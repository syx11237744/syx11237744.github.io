<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Markov Decision Process (MDP) 1. 马尔可夫过程 通常用元组 \(⟨S,P⟩\) 描述一个马尔可夫过程，其中 \(S\) 为有限数量的状态集合，\(P\) 是状态转移矩阵： \[ P &#x3D; \begin{bmatrix} P(s_1|s_1) &amp; \cdots &amp; P(s_n|s_1) \\ \vdots &amp; \ddots &amp; \vdot">
<meta property="og:type" content="article">
<meta property="og:title" content="RL学习笔记">
<meta property="og:url" content="http://example.com/2024/10/03/RL/index.html">
<meta property="og:site_name" content="Xuu&#39;s Blog">
<meta property="og:description" content="Markov Decision Process (MDP) 1. 马尔可夫过程 通常用元组 \(⟨S,P⟩\) 描述一个马尔可夫过程，其中 \(S\) 为有限数量的状态集合，\(P\) 是状态转移矩阵： \[ P &#x3D; \begin{bmatrix} P(s_1|s_1) &amp; \cdots &amp; P(s_n|s_1) \\ \vdots &amp; \ddots &amp; \vdot">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-10-03T09:50:41.047Z">
<meta property="article:modified_time" content="2024-10-03T10:03:03.676Z">
<meta property="article:author" content="Xuu">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2024/10/03/RL/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>RL学习笔记 | Xuu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xuu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/10/03/RL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xuu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xuu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RL学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2024-10-03 17:50:41 / Modified: 18:03:03" itemprop="dateCreated datePublished" datetime="2024-10-03T17:50:41+08:00">2024-10-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h2>
<h3 id="马尔可夫过程">1. 马尔可夫过程</h3>
<p>通常用元组 <span class="math inline">\(⟨S,P⟩\)</span>
描述一个马尔可夫过程，其中 <span class="math inline">\(S\)</span>
为有限数量的状态集合，<span class="math inline">\(P\)</span>
是状态转移矩阵：</p>
<p><span class="math display">\[
P = \begin{bmatrix}
P(s_1|s_1) &amp; \cdots &amp; P(s_n|s_1) \\
\vdots &amp; \ddots &amp; \vdots \\
P(s_1|s_n) &amp; \cdots &amp; P(s_n|s_n)
\end{bmatrix}
\]</span></p>
<p>对于状态转移矩阵，每一行的和都是
1。直观地理解，每个状态一定会到下一个状态，那么把所有可能到达的状态的概率求和，就为
1。 <span id="more"></span> ### 2. 马尔可夫奖励过程</p>
<p>在马尔可夫过程中加入奖励函数 <span class="math inline">\(r\)</span>
和折扣因子 <span
class="math inline">\(\gamma\)</span>，就可以得到马尔可夫奖励过程（Markov
Reward Process）。一个马尔可夫奖励过程由 <span class="math inline">\((S,
P, r, \gamma)\)</span> 构成，各个组成元素的含义如下：</p>
<ul>
<li><span class="math inline">\(S\)</span> 是有限状态的集合。</li>
<li><span class="math inline">\(P\)</span> 是状态转移矩阵。</li>
<li><span class="math inline">\(r\)</span> 是奖励函数，某个状态 <span
class="math inline">\(s\)</span> 的奖励 <span
class="math inline">\(r(s)\)</span>
指转移到该状态时可以获得奖励的期望。</li>
<li><span class="math inline">\(\gamma\)</span> 是折扣因子（discount
factor），<span class="math inline">\(\gamma\)</span> 的取值范围为 [0,
1)。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近
1 的 <span class="math inline">\(\gamma\)</span>
更关注长期的累计奖励，接近 0 的 <span
class="math inline">\(\gamma\)</span> 更考虑短期奖励。</li>
</ul>
<h4 id="回报">2.1 回报</h4>
<p><span class="math display">\[
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots =
\sum_{k=0}^\infty \gamma^k R_{t+k}
\]</span></p>
<p>其中，<span class="math inline">\(R_t\)</span> 表示在时刻 <span
class="math inline">\(t\)</span> 获得的奖励。</p>
<h4 id="价值函数">2.2 价值函数</h4>
<p>一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value），价值函数的定义
<span class="math inline">\(V(s) = \mathbb{E}[G_t \mid S_t =
s]\)</span>，展开为：</p>
<p><span class="math display">\[
V(s) = \mathbb{E}[G_t \mid S_t = s]
= \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots \mid S_t =
s]
= \mathbb{E}[R_t + \gamma G_{t+1} \mid S_t = s]
= \mathbb{E}[R_t \mid S_t = s] + \gamma \mathbb{E}[G_{t+1} \mid S_t = s]
= r(s) + \gamma \mathbb{E}[V(S_{t+1}) \mid S_t = s]
\]</span></p>
<p><span class="math inline">\(\mathbb{E}[R_t \mid S_t = s] =
r(s)\)</span>；另一方面，等式中剩余部分 <span
class="math inline">\(\mathbb{E}[\gamma V(S_{t+1}) \mid S_t =
s]\)</span> 可以根据从状态 <span class="math inline">\(s\)</span>
出发的转移概率得到，即可以得到：</p>
<p><span class="math display">\[
V(s) = r(s) + \gamma \sum_{s&#39; \in S} p(s&#39; \mid s)V(s&#39;)
\]</span></p>
<p>上面就是贝尔曼方程（Bellman
Equation）。将贝尔曼方程写成矩阵的形式：</p>
<p><span class="math display">\[
\mathbf{V} = \mathbf{R} + \gamma \mathbf{P} \mathbf{V}
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
V(s_1) \\
V(s_2) \\
\vdots \\
V(s_n)
\end{bmatrix}
=
\begin{bmatrix}
r(s_1) \\
r(s_2) \\
\vdots \\
r(s_n)
\end{bmatrix}
+
\gamma
\begin{bmatrix}
p(s_1 \mid s_1) &amp; \cdots &amp; p(s_n \mid s_1) \\
p(s_1 \mid s_2) &amp; \cdots &amp; p(s_n \mid s_2) \\
\vdots &amp; \ddots &amp; \vdots \\
p(s_1 \mid s_n) &amp; \cdots &amp; p(s_n \mid s_n)
\end{bmatrix}
\begin{bmatrix}
V(s_1) \\
V(s_2) \\
\vdots \\
V(s_n)
\end{bmatrix}
\]</span></p>
<p>我们可以直接根据矩阵运算求解，得到以下解析解：</p>
<p><span class="math display">\[
\mathbf{V} = (\mathbf{I} - \gamma \mathbf{P})^{-1} \mathbf{R}
\]</span></p>
<h3 id="马尔可夫决策过程">3. 马尔可夫决策过程</h3>
<p>上面的过程都是自发的，对于 MP 过程是基于概率自发的，对于 MRP
过程基于概率和累积奖励自发，对于马尔可夫决策过程（Markov Decision
Process,
MDP），他接受来自外界的刺激，称作<strong>智能体的动作</strong>。与 MRP
的过程十分相似，只是多了一个动作的变量，具体而言，由 <span
class="math inline">\(⟨S,A,P,r,\lambda⟩\)</span> 构成：</p>
<ul>
<li><span class="math inline">\(S\)</span> 是状态的集合</li>
<li><span class="math inline">\(A\)</span> 是动作的集合</li>
<li><span class="math inline">\(\lambda\)</span> 是折扣因子</li>
<li><span class="math inline">\(r(s,a)\)</span> 是奖励函数</li>
<li><span class="math inline">\(P(s&#39; \mid s,a)\)</span>
是状态转移函数，表示在 <span class="math inline">\(s\)</span>
状态执行动作 <span class="math inline">\(a\)</span> 之后到达状态 <span
class="math inline">\(s&#39;\)</span> 的概率</li>
</ul>
<h4 id="策略">3.1 策略</h4>
<p>智能体的策略 (Policy) 通常使用 <span
class="math inline">\(\pi\)</span> 来表示。<span
class="math inline">\(\pi(a \mid s)=P(A_t=a \mid S_t=s)\)</span>
是一个函数，表示在输入状态 <span class="math inline">\(s\)</span>
情况下采取动作 <span class="math inline">\(a\)</span>
的概率。分为确定性策略（对于一个状态之后只会执行一个特定的动作）和随机性策略（对于一个状态会得到的是一个概率分布）。</p>
<h4 id="状态价值函数">3.2 状态价值函数</h4>
<p><span class="math inline">\(V^{\pi}(s)=E_{\pi}[G_t \mid
S_t=s]\)</span> 定义为从状态 <span class="math inline">\(s\)</span>
出发，按照策略 <span class="math inline">\(\pi\)</span>
能获得的期望回报。</p>
<h4 id="动作价值函数">3.3 动作价值函数</h4>
<p><span class="math inline">\(Q^{\pi}(s,a)=E_{\pi}[G_t \mid S_t=s,
A_t=a]\)</span> 表示在 MDP 遵循策略 <span
class="math inline">\(\pi\)</span> 的时候，对于当前动作 <span
class="math inline">\(s\)</span> 执行动作 <span
class="math inline">\(a\)</span> 得到的期望回报。</p>
<p>由全概率公式可以得到：</p>
<p><span class="math display">\[
V^{\pi}(s)=\sum_{a\in A}\pi(a \mid s)Q^{\pi}(s,a)
\]</span></p>
<p>类似于 MRP，可以得到下面的公式：</p>
<p><span class="math display">\[
Q^{\pi}(s,a)=E_{\pi}[R_t+\lambda Q^{\pi}(S_{t+1},A_{t+1}) \mid
S_t=s,A_t=a]
\]</span></p>
<p><span class="math display">\[
=r(s,a)+\lambda \sum_{s&#39;\in S}(p(s&#39; \mid s,a)\sum_{a&#39;\in
A}\pi(a&#39; \mid s&#39;)Q^{\pi}(s&#39;,a&#39;))
\]</span></p>
<h4 id="贝尔曼期望方程">3.4 贝尔曼期望方程</h4>
<p><span class="math display">\[
V^{\pi}(s)=E_{\pi}[G_t \mid S_t=s]=\sum_{a\in A}\left(\pi(a \mid
s)\left(r(s,a)+\lambda\sum_{s&#39;}p(s&#39; \mid
s,a)V^{\pi}(s&#39;)\right)\right)
\]</span></p>
<p><span class="math display">\[
Q^{\pi}(s,a)=r(s,a)+\lambda \sum_{s&#39;\in S}\left(p(s&#39; \mid
s,a)\sum_{a&#39;\in A}\pi(a&#39; \mid
s&#39;)Q^{\pi}(s&#39;,a&#39;)\right)
\]</span></p>
<h4 id="占用度量">3.5 占用度量</h4>
<p>首先，我们定义 MDP 的初始状态分布 <span
class="math inline">\(V_0(s)\)</span>，用 <span
class="math inline">\(P_t^{\pi}\)</span> 表示采取策略 <span
class="math inline">\(\pi\)</span> 使得智能体在 <span
class="math inline">\(t\)</span> 时刻状态为 <span
class="math inline">\(s\)</span>
的概率，我们可以定义一个状态访问分布（它描述了在整个时间过程中，智能体有多大概率访问到某个特定状态）：</p>
<p><span class="math display">\[
v^{\pi}(s)=(1-\lambda)\sum_{t=0}^{\infty}\lambda^{t}P_t^{\pi}(s)
\]</span></p>
<p>此外，我们还可以定义策略的占用度量（occupancy measure）：</p>
<p><span class="math display">\[
\rho^\pi(s, a) = (1 - \gamma) \sum_{t=0}^{\infty} \gamma^t P_t^\pi(s)
\pi(a \mid s)
\]</span></p>
<p>它表示动作状态对 <span class="math inline">\((s, a)\)</span>
被访问到的概率。二者之间存在如下关系：</p>
<p><span class="math display">\[
\rho^\pi(s, a) = v^\pi(s) \pi(a \mid s)
\]</span></p>
<p>进一步得出如下两个定理。<strong>定理 1</strong>：智能体分别以策略
<span class="math inline">\(\pi_1\)</span> 和 <span
class="math inline">\(\pi_2\)</span> 和同一个 MDP 交互得到的占用度量
<span class="math inline">\(\rho^{\pi_1}\)</span> 和 <span
class="math inline">\(\rho^{\pi_2}\)</span> 满足：</p>
<p><span class="math display">\[
\rho^{\pi_1} = \rho^{\pi_2} \iff \pi_1 = \pi_2
\]</span></p>
<p><strong>定理 2</strong>：给定一个合法占用度量 <span
class="math inline">\(\rho\)</span>，可生成该占用度量的唯一策略是：</p>
<p><span class="math display">\[
\pi_\rho = \frac{\rho(s, a)}{\sum_{a&#39;} \rho(s, a&#39;)}
\]</span></p>
<h4 id="最优策略">3.6 最优策略</h4>
<p>强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。我们首先定义策略之间的偏序关系：当且仅当对于任意的状态
<span class="math inline">\(s\)</span> 都有 <span
class="math inline">\(V^{\pi&#39;}(s) \ge V^\pi(s)\)</span> 时，记 <span
class="math inline">\(\pi&#39; \succeq
\pi\)</span>。于是存在有限状态和动作集合的 MDP
中，至少存在一个策略比其他所有策略都好或者至少不差于其他所有策略，这个策略就是最优策略（optimal
policy）。最优策略可能有很多个，我们都将其表示为 <span
class="math inline">\(\pi^*\)</span>。</p>
<p>最优策略都有相同的状态价值函数，我们称之为<strong>最优状态价值函数</strong>，表示为：</p>
<p><span class="math display">\[
V^*(s) = \max_\pi V^\pi(s), \quad \forall s \in S
\]</span></p>
<p>同理，我们定义<strong>最优动作价值函数</strong>为：</p>
<p><span class="math display">\[
Q^*(s, a) = \max_\pi Q^\pi(s, a), \quad \forall s \in S, a \in A
\]</span></p>
<p>为了使 <span class="math inline">\(Q^\pi(s, a)\)</span>
最大，我们需要在当前的状态动作对 <span class="math inline">\((s,
a)\)</span>
之后都执行最优策略。于是我们得到了最优状态价值函数和最优动作价值函数之间的关系：</p>
<p><span class="math display">\[
Q^*(s, a) = r(s, a) + \gamma \sum_{s&#39; \in S} P(s&#39; \mid s, a)
V^*(s&#39;)
\]</span></p>
<p>这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择使得最优动作价值最大的那个动作时的状态价值：</p>
<p><span class="math display">\[
V^*(s) = \max_{a \in A} Q^*(s, a)
\]</span></p>
<h4 id="贝尔曼最优方程">3.7 贝尔曼最优方程</h4>
<p>根据 <span class="math inline">\(V^*(s)\)</span> 和 <span
class="math inline">\(Q^*(s, a)\)</span>
的关系，我们可以得到<strong>贝尔曼最优方程</strong>（Bellman Optimality
Equation）：</p>
<p><span class="math display">\[
V^*(s) = \max_{a \in A} \{ r(s, a) + \gamma \sum_{s&#39; \in S} p(s&#39;
\mid s, a) V^*(s&#39;) \}
\]</span></p>
<p><span class="math display">\[
Q^*(s, a) = r(s, a) + \gamma \sum_{s&#39; \in S} P(s&#39; \mid s, a)
\max_{a&#39; \in A} Q^*(s&#39;, a&#39;)
\]</span></p>
<h2 id="动态规划算法">动态规划算法</h2>
<h3 id="策略迭代算法">1. 策略迭代算法</h3>
<h4 id="策略评估">1.1 策略评估</h4>
<p>随机选定<span
class="math inline">\(V^0\)</span>,在贝尔曼期望期望方程上迭代： <span
class="math display">\[
V^{k+1}(s)=\sum_{a\in A}\pi(a|s)\left(r(s|a)+\gamma\sum_{s&#39;\in
S}P(s&#39;|s,a)V^k(s&#39;) \right)
\]</span> （数学原理可以保证这个的成立）</p>
<h4 id="策略提升">1.2 策略提升</h4>
<p>使用完全贪心的策略,直接贪心的在每个状态选择动作价值最大的那个动作
<span class="math display">\[
\pi&#39;(s)=arg max_aQ^{\pi}(s,a)
\]</span></p>
<h4 id="策略迭代算法-1">1.3 策略迭代算法</h4>
<p>对当前的策略进行策略评估，得到其状态价值函数，然后根据该状态价值函数进行策略提升以得到一个更好的新策略，接着继续评估新策略、提升策略……直至最后收敛到最优策略</p>
<h4 id="价值迭代算法">1.4 价值迭代算法</h4>
<p>在贝尔曼最优方程上进行迭代 <span class="math display">\[
V^{k+1}(s)=max_{a\in A}\{r(s|a)+\gamma\sum_{s&#39;\in
S}P(s&#39;|s,a)V^k(s&#39;)\}
\]</span></p>
<h2 id="时序差分算法">时序差分算法</h2>
<p><strong>无模型的强化学习</strong>：对于大部分的强化学习现实场景，环境的奖励函数和状态转移函数是未知的，因此智能体只能通过和环境进行交互，通过采样到的数据进行学习</p>
<h3 id="时序差分方法">1. 时序差分方法</h3>
<h4 id="增量更新">1.1增量更新：</h4>
<p><span class="math display">\[
\begin{aligned}
Q_k &amp;= \frac{1}{k} \sum_{i=1}^k r_i \\
&amp;= \frac{1}{k} \left( r_k + \sum_{i=1}^{k-1} r_i \right) \\
&amp;= \frac{1}{k} \left( r_k + (k-1)Q_{k-1} \right) \\
&amp;= \frac{1}{k} \left( r_k + kQ_{k-1} - Q_{k-1} \right) \\
&amp;= Q_{k-1} + \frac{1}{k} \left[ r_k - Q_{k-1} \right]
\end{aligned}
\]</span></p>
<p>把<span
class="math inline">\(Q_k\)</span>是一种平均值，假设我们的<span
class="math inline">\(r_i\)</span>是逐渐得到的，如果我们等到得到最后一个r的时候再去计算平均值就比较慢，因此，我们可以按照时序顺序来计算，用之前的平均值，加上当前的值与平均值的差距，再乘以一个小因子，相当于每次都给之前的平均值做一个调整。</p>
<h4 id="公式">1.2 公式</h4>
<p><span class="math display">\[
V(s_t)\leftarrow V(s_t)+\alpha[r_t+\gamma V(s_{t+1}-V(s_t)]
\]</span></p>
<h3 id="sarsa算法">2. Sarsa算法</h3>
<p>策略评估已经可以通过时序差分算法实现了，类似的我们可以使用时序差分算法去估计动作价值函数Q:
<span class="math display">\[
Q(s_t,a_t)\leftarrow
Q(s_t,a_t)+\alpha[r_t+Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]
\]</span> 如果一直进行完全贪婪选择的话，可能会导致某些状态动作<span
class="math inline">\((s,a)\)</span>永远没有在序列中出现，导致无法对其动作价值进行估计，因此我们采用<span
class="math inline">\(\epsilon\)</span>贪婪策略</p>
<h3 id="q-learning算法">3. Q-learning算法</h3>
<p>Q-learning基于<strong>Bellman方程</strong>进行更新。</p>
<p><span class="math display">\[
Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \cdot
\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s, a) \right)
\]</span></p>
<ul>
<li><span class="math inline">\(\max_{a&#39;} Q(s&#39;,
a&#39;)\)</span>：在下一个状态 ( s' )
下，所有可能动作中Q值最大的动作对应的Q值。</li>
</ul>
<h2 id="dqn算法">DQN算法</h2>
<h3 id="dqn">DQN</h3>
<p>参考公式30，我们的目标是使<span
class="math inline">\(Q(s,a)\)</span>和TD的目标<span
class="math inline">\(r+\gamma max_{a&#39; \in
A}Q(s&#39;,a&#39;)\)</span>靠近，于是 <span class="math display">\[
w^*=arg min_{w}\frac{1}{2N}\sum^N_{i=1}\left[Q_w(s_i,a_i)-\left( r +
\gamma \cdot \max_{a&#39;} Q(s&#39;, a&#39;) - Q(s, a) \right)\right]
\]</span></p>
<h3 id="经验回放">经验回放</h3>
<p>当前状态和上一个状态紧密相关，如果直接使用连续的样本进行训练，数据之间会高度相关，无法满足神经网络训练所需的独立同分布假设，通过经验回放（在训练神经网络的时候不使用agent最近收集的数据，而是从buffer中随机采样若干数据进行训练）打破样本之间的相关性，减少对神经网络训练的负面影响。</p>
<h3 id="目标网络">目标网络</h3>
<p>将损失函数分为两个部分，一个后半部分每次都更新，一个固定住</p>
<h3 id="double-dqn">Double DQN</h3>
<p>传统的DQN容易造成过高的估计，于是改写为 <span class="math display">\[
r+\gamma Q_{w^-}\left(s&#39;,arg max_{a&#39;}Q_w(s&#39;,a&#39;)\right)
\]</span> 其中目标网络的参数记做<span
class="math inline">\(w^-\)</span>,训练网络的参数记做<span
class="math inline">\(w\)</span></p>
<h3 id="dueling-dqn">Dueling DQN</h3>
<p>在 Dueling DQN 中，Q 网络被建模为： <span class="math display">\[
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)
\]</span> <img
src="/Users/sunyuanxu/Library/Application%20Support/typora-user-images/image-20240907180441685.png"
alt="image-20240907180441685" /></p>
<p>分别建模的好处：某些情境下智能体只会关注状态的价值，而并不关心不同动作导致的差异，此时将二者分开建模能够使智能体更好地处理与动作关联较小的状态。</p>
<p>对于这样的Q值的计算，同一个Q值可能对应不同的V，A，因此Dueling DQN
强制最优动作的优势函数的实际输出为 0： <span class="math display">\[
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)-max_{a&#39;}A_{\eta,\beta}(s,a&#39;)
\]</span> 也可以使用平均代替最大化操作： <span class="math display">\[
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)-\frac{1}{|A|}\sum_{a&#39;}A_{\eta,\beta}(s,a&#39;)
\]</span> 其中<span
class="math inline">\(｜A｜\)</span>表示动作空间中所有可能的动作数量。</p>
<h2 id="策略梯度算法">策略梯度算法</h2>
<p>基于策略的方法则是直接显式的学习一个目标策略</p>
<h3 id="策略梯度">策略梯度</h3>
<p>基于策略的方法首先需要将策略参数化。假设目标策略<span
class="math inline">\(\pi_{\theta}\)</span>是一个随机性策略，并且处处可微，将策略学习的目标函数定义为<span
class="math inline">\(J(\theta)=E_{s_0}[V^{\pi_{\theta}}(s_0)]\)</span>，对其求导，然后使用梯队上升方法来最大化这个目标函数。
<span class="math display">\[
\nabla_{\theta}
J(\theta)=E_{\pi_{\theta}}[Q^{\pi_{\theta}}(s,a)\nabla_{\theta}\ log\
\pi_{\theta}(a|s)]
\]</span>
直观的理解就是增大能够得到较高Q值的动作的概率，减少较低Q值的动作的概率</p>
<p>Reinforce算法：有限步数环境下采样轨迹计算相应的价值，利用蒙特卡洛采样
<span class="math display">\[
\nabla_{\theta}
J(\theta)=E_{\pi_{\theta}}\left[\sum^T_{t=0}\left(\sum^T_{t&#39;=t}\gamma^{t&#39;-t}r_{t&#39;}\right)\nabla_{\theta}\
log\ \pi_{\theta}(a|s)\right]
\]</span> 使用当前策略采样，计算当前轨迹每个时刻t往后的回报，记作<span
class="math inline">\(\psi_t\)</span>,然后梯度上升 <span
class="math display">\[
\theta \leftarrow \theta+\alpha\sum_{t}^T\psi_t\nabla_{\theta}\ log\
\pi_{\theta}(a|s)
\]</span></p>
<h2 id="actor-critic算法">Actor-Critic算法</h2>
<p>优化一个带参数的策略，并且额外学习价值函数，帮助策略函数更好的学习</p>
<ul>
<li><p>Actor学习策略 <span class="math display">\[
\theta \leftarrow \theta+\alpha\sum_{t}^T\psi_t\nabla_{\theta}\ log\
\pi_{\theta}(a|s)
\]</span> 其中<span class="math inline">\(\psi_t\)</span>有多种</p></li>
<li><p>Critic学习价值函数，类似DQN，定义损失函数如下 <span
class="math display">\[
L(w)=\frac{1}{2}(r+\gamma V_{w}(s_{t+1})-V_w(s_t))^2
\]</span></p></li>
</ul>
<p>​ 梯度</p>
<p>​<br />
<span class="math display">\[
\nabla_w L(w)=-(r+\gamma V_{w}(s_{t+1})-V_w(s_t))\nabla_w V_w(s_t)
\]</span></p>
<h2 id="trpo算法">TRPO算法</h2>
<p>策略优化算法通过直接优化策略来找到最优策略，通常使用梯度下降的方法，然而有时会导致策略更新过大，从而使得新策略的性能不稳定甚至变差。因此需要保证新策略不会偏离当前策略太远，进而使得每次更新后策略的性能稳定提升。</p>
<p>定义新旧策略的目标函数的差距： <span class="math display">\[
\begin{align}
J(\theta&#39;)-J(\theta)&amp;=E_{\pi_{\theta&#39;}}\left[\sum^{\infty}_{t=0}\gamma^t[r(s_t,s_t)+\gamma
V^{\pi_{\theta}}(s_{t+1})-V^{\pi_{\theta}}(s_{t})]\right]\\
&amp;:=E_{\pi_{\theta&#39;}}\left[\sum^{\infty}_{t=0}\gamma^t
A^{\pi_{\theta}}(s_t,a_t) \right]\\
&amp;=\frac{1}{1-\gamma}E_{s\sim v^{\pi_{\theta&#39;}}}E_{a\sim
\pi_{\theta&#39;}(·|s)}[A^{\pi_{\theta}}(s,a)]
\end{align}
\]</span> 利用<span
class="math inline">\(v^{\pi_{\theta}}(s)=(1-\gamma)\sum^{\infty}_{t=0}\gamma^tP^{\pi}_{t}(s)\)</span>因此，只要新的策略保证<span
class="math inline">\(E_{s\sim v^{\pi_{\theta&#39;}}}E_{a\sim
\pi_{\theta&#39;}}(·|s)[A^{\pi_{\theta}}(s,a)]\geq 0\)</span>即可。</p>
<p>假设：新旧策略非常接近，状态访问分布变化小。因此直接采用旧策略的状态分布，得到替代的优化目标
<span class="math display">\[
L_{\theta}(\theta&#39;)=J(\theta)+E_{s\sim v^{\pi_{\theta}}}E_{a\sim
\pi_{\theta}(·|s)}[\frac{\pi_{\theta&#39;}(a|s)}{\pi_{\theta}(a|s)}A^{\pi_{\theta}}(s,a)]
\]</span> 为了限制新旧策略，因此引入KL散度，整体的优化公式 <span
class="math display">\[
max_{\theta&#39;}L_{\theta}(\theta&#39;) s.t.E_{s\sim
v^{\pi_{\theta}}}\left[D_{KL}(\pi_{\theta},\pi_{\theta&#39;})\right]
\]</span></p>
<h3 id="近似求解">近似求解</h3>
<p>参考<a
target="_blank" rel="noopener" href="https://hrl.boyuai.com/chapter/2/trpo%E7%AE%97%E6%B3%95">动手学强化学习</a></p>
<h2 id="ppo">PPO</h2>
<p>直接将KL散度放到目标函数中， <span class="math display">\[
arg\ max_{\theta&#39;}E_{s\sim v^{\pi_{\theta}}}E_{a\sim
\pi_{\theta}(·|s)}[\frac{\pi_{\theta&#39;}(a|s)}{\pi_{\theta}(a|s)}A^{\pi_{\theta}}(s,a)-\beta
D_{KL}(\pi_{\theta},\pi_{\theta&#39;})]
\]</span></p>
<h3 id="ppo惩罚">PPO惩罚</h3>
<p>令<span
class="math inline">\(d_k=D_{KL}^{v^{\pi_{\theta}}}(\pi_{\theta},\pi_{\theta&#39;})\)</span>,<span
class="math inline">\(\beta\)</span>的更新规则如下： <span
class="math display">\[
\beta_{k+1}=
\begin{cases}
\frac{\beta_k}{2},\ 如果\ d_k &lt; \frac{\delta}{1.5} \\
\beta_k*2,\ 如果\ d_k &gt; \delta*1.5 \\
\beta_k, 否则\\
\end{cases}
\]</span></p>
<h3 id="ppo截断">PPO截断</h3>
<p><span class="math display">\[
\arg\max_{\theta} \mathbb{E}_{s \sim \nu} \mathbb{E}_{a \sim
\pi_{\theta_k}(\cdot|s)} \left[ \min \left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s, a),
\text{clip} \left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, 1 -
\epsilon, 1 + \epsilon \right) A^{\pi_{\theta_k}}(s, a) \right) \right]
\]</span></p>
<p>相当于限制住优势函数，如果<span
class="math inline">\(A^{\pi_{\theta_k}}(s,
a)&gt;0\)</span>，最大化这个式子会增加<span
class="math inline">\(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}\)</span>，但是不会超过<span
class="math inline">\(1+\epsilon\)</span>，如果<span
class="math inline">\(A^{\pi_{\theta_k}}(s,
a)&lt;0\)</span>，最大化这个式子会减小<span
class="math inline">\(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}\)</span>，但是不会超过<span
class="math inline">\(1-\epsilon\)</span>。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/10/03/NLP/" rel="prev" title="NLP">
      <i class="fa fa-chevron-left"></i> NLP
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#markov-decision-process-mdp"><span class="nav-number">1.</span> <span class="nav-text">Markov Decision Process (MDP)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B"><span class="nav-number">1.1.</span> <span class="nav-text">1. 马尔可夫过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%9E%E6%8A%A5"><span class="nav-number">1.1.1.</span> <span class="nav-text">2.1 回报</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.2.</span> <span class="nav-text">2.2 价值函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="nav-number">1.2.</span> <span class="nav-text">3. 马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5"><span class="nav-number">1.2.1.</span> <span class="nav-text">3.1 策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.2.</span> <span class="nav-text">3.2 状态价值函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.3.</span> <span class="nav-text">3.3 动作价值函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%9F%E6%9C%9B%E6%96%B9%E7%A8%8B"><span class="nav-number">1.2.4.</span> <span class="nav-text">3.4 贝尔曼期望方程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%A0%E7%94%A8%E5%BA%A6%E9%87%8F"><span class="nav-number">1.2.5.</span> <span class="nav-text">3.5 占用度量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5"><span class="nav-number">1.2.6.</span> <span class="nav-text">3.6 最优策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B"><span class="nav-number">1.2.7.</span> <span class="nav-text">3.7 贝尔曼最优方程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">动态规划算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="nav-number">2.1.</span> <span class="nav-text">1. 策略迭代算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="nav-number">2.1.1.</span> <span class="nav-text">1.1 策略评估</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%8F%90%E5%8D%87"><span class="nav-number">2.1.2.</span> <span class="nav-text">1.2 策略提升</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95-1"><span class="nav-number">2.1.3.</span> <span class="nav-text">1.3 策略迭代算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="nav-number">2.1.4.</span> <span class="nav-text">1.4 价值迭代算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">时序差分算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.</span> <span class="nav-text">1. 时序差分方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A2%9E%E9%87%8F%E6%9B%B4%E6%96%B0"><span class="nav-number">3.1.1.</span> <span class="nav-text">1.1增量更新：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F"><span class="nav-number">3.1.2.</span> <span class="nav-text">1.2 公式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sarsa%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">2. Sarsa算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q-learning%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.</span> <span class="nav-text">3. Q-learning算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dqn%E7%AE%97%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">DQN算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dqn"><span class="nav-number">4.1.</span> <span class="nav-text">DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="nav-number">4.2.</span> <span class="nav-text">经验回放</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E7%BD%91%E7%BB%9C"><span class="nav-number">4.3.</span> <span class="nav-text">目标网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#double-dqn"><span class="nav-number">4.4.</span> <span class="nav-text">Double DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dueling-dqn"><span class="nav-number">4.5.</span> <span class="nav-text">Dueling DQN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="nav-number">5.</span> <span class="nav-text">策略梯度算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">5.1.</span> <span class="nav-text">策略梯度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#actor-critic%E7%AE%97%E6%B3%95"><span class="nav-number">6.</span> <span class="nav-text">Actor-Critic算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#trpo%E7%AE%97%E6%B3%95"><span class="nav-number">7.</span> <span class="nav-text">TRPO算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%91%E4%BC%BC%E6%B1%82%E8%A7%A3"><span class="nav-number">7.1.</span> <span class="nav-text">近似求解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ppo"><span class="nav-number">8.</span> <span class="nav-text">PPO</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ppo%E6%83%A9%E7%BD%9A"><span class="nav-number">8.1.</span> <span class="nav-text">PPO惩罚</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ppo%E6%88%AA%E6%96%AD"><span class="nav-number">8.2.</span> <span class="nav-text">PPO截断</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Xuu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xuu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  















    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

    </div>
</body>
</html>
